{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import soundfile as sf\n",
    "#from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration, MusicgenForConditionalGeneration\n",
    "from diffusers import StableAudioPipeline\n",
    "import numpy as np\n",
    "import sys, torch\n",
    "from tqdm import tqdm as notebook_tqdm\n",
    "\n",
    "MAX_TOKENS: int = 1503 # 30 seconds of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableAudioPipeline.from_pretrained(\"stabilityai/stable-audio-open-1.0\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda:1\")\n",
    "\n",
    "# define the prompts\n",
    "prompt = \"a calm creek with a gentle stream, birds chirping, and a soft breeze\"\n",
    "negative_prompt = \"Low quality.\"\n",
    "\n",
    "# set the seed for generator\n",
    "generator = torch.Generator(\"cuda:1\").manual_seed(0)\n",
    "\n",
    "num_waveforms: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the generation\n",
    "audio = pipe(\n",
    "    prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=200,\n",
    "    audio_end_in_s=10.0,\n",
    "    num_waveforms_per_prompt=num_waveforms,\n",
    "    generator=generator,\n",
    ").audios\n",
    "\n",
    "print(audio.shape)\n",
    "\n",
    "for i in range(num_waveforms):\n",
    "    output = audio.squeeze(0).T.float().cpu().numpy()\n",
    "    sf.write(f\"sounds/test.wav\", output, pipe.vae.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(441000, 2) 44100\n"
     ]
    }
   ],
   "source": [
    "a, sr = sf.read('sounds/sound_0_1.wav', dtype='float32')\n",
    "print(a.shape, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings + textual inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableAudioPipeline\n",
    "import laion_clap\n",
    "import soundfile as sf\n",
    "import accelerate\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the CLAP_Module without loading\n",
    "model = laion_clap.CLAP_Module(enable_fusion=False, amodel=\"HTSAT-base\").to(\"cuda:1\")\n",
    "\n",
    "model.load_ckpt()\n",
    "\n",
    "# # 2. Load the raw checkpoint\n",
    "# ckpt = torch.load(\"630k-audioset-best.pt\", map_location=\"cuda:1\")\n",
    "\n",
    "# # 3. Remove the rogue key(s)\n",
    "# ckpt_clean = {k: v for k, v in ckpt.items()\n",
    "#               if \"text_branch.embeddings.position_ids\" not in k}\n",
    "\n",
    "# # 4. Manually load into the underlying PyTorch model\n",
    "# model.model.load_state_dict(ckpt_clean, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.transformers.stable_audio_transformer import StableAudioDiTModel\n",
    "import sys\n",
    "sys.path.append('./src/')\n",
    "from diffusers.pipelines.stable_audio import StableAudioPipeline\n",
    "\n",
    "transformer = StableAudioDiTModel.from_pretrained(\n",
    "        \"stabilityai/stable-audio-open-1.0\",\n",
    "        subfolder=\"transformer\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "pipe = StableAudioPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-audio-open-1.0\",\n",
    "        torch_dtype=torch.float16,\n",
    "        transformer=transformer,\n",
    "    )\n",
    "\n",
    "pipe = pipe.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = torch.nn.Linear(512, 768).to(\"cuda:1\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Audio embedding\n",
    "\n",
    "audio_file = [\n",
    "    'sounds/sound_0_1.wav',\n",
    "]\n",
    "audio_embed = projector(model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=True)).unsqueeze(0).to(torch.float16)\n",
    "print('Audio -> ', audio_embed.shape)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# text embedding\n",
    "\n",
    "text_data = [\"high quality version of X\"]\n",
    "text_embed = projector(model.get_text_embedding(text_data, use_tensor=True)).unsqueeze(0).to(torch.float16)\n",
    "print('Text -> ', text_embed.shape)\n",
    "\n",
    "print(text_embed.dtype, audio_embed.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pipe.to(\"cuda:1\")\n",
    "\n",
    "text_embed = text_embed.to(\"cuda:1\")\n",
    "audio_embed = audio_embed.to(\"cuda:1\")\n",
    "print(text_embed.shape, audio_embed.shape)\n",
    "\n",
    "# use __call__ to generate\n",
    "audio = pipe.__call__(\n",
    "    num_inference_steps=200,\n",
    "    audio_end_in_s=10.0,\n",
    "    num_waveforms_per_prompt=num_waveforms,\n",
    "    generator=generator,\n",
    "    prompt_embeds=audio_embed + text_embed, # add the audio and text embeddings \n",
    ").audios\n",
    "\n",
    "# out.audios is a list of tensors; take the first one\n",
    "print(audio.shape)\n",
    "generated = audio.squeeze(0).T.float().cpu().numpy()\n",
    "\n",
    "# 5. Save it\n",
    "sf.write(\"sounds/something.wav\", generated, samplerate=pipe.vae.sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
