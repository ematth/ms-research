{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea77ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ClapProcessor, ClapModel\n",
    "from diffusers import StableAudioPipeline\n",
    "import soundfile as sf\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2a3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load CLAP and extract both audio- and text-embeddings :contentReference[oaicite:0]{index=0}\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-fused\").to(device)\n",
    "processor  = ClapProcessor.from_pretrained(\"laion/clap-htsat-fused\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2993cbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f82169ea3244a7a2d56b86bfa634b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evanmm3/ms-research/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "# 2. Load StableAudioPipeline :contentReference[oaicite:1]{index=1}\n",
    "pipe = StableAudioPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-audio-open-1.0\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c8d31b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 3. Get your CLAP audio embedding\u001b[39;00m\n\u001b[1;32m      2\u001b[0m audio, sr \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msounds/test_48k.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m clap_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      5\u001b[0m     audio_emb_clap \u001b[38;5;241m=\u001b[39m clap_model\u001b[38;5;241m.\u001b[39mget_audio_features(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclap_inputs)\u001b[38;5;241m.\u001b[39maudio_embeds  \u001b[38;5;66;03m# (1, clap_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/ms-research/venv/lib/python3.10/site-packages/transformers/models/clap/processing_clap.py:87\u001b[0m, in \u001b[0;36mClapProcessor.__call__\u001b[0;34m(self, text, audios, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audios \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     audio_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudios\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m audios \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     encoding\u001b[38;5;241m.\u001b[39mupdate(audio_features)\n",
      "File \u001b[0;32m~/ms-research/venv/lib/python3.10/site-packages/transformers/models/clap/feature_extraction_clap.py:334\u001b[0m, in \u001b[0;36mClapFeatureExtractor.__call__\u001b[0;34m(self, raw_speech, truncation, padding, max_length, sampling_rate, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39masarray(raw_speech)]\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# convert to mel spectrogram, truncate and pad if needed.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_input_mel(waveform, max_length \u001b[38;5;28;01mif\u001b[39;00m max_length \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_max_samples, truncation, padding)\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m waveform \u001b[38;5;129;01min\u001b[39;00m raw_speech\n\u001b[1;32m    337\u001b[0m ]\n\u001b[1;32m    339\u001b[0m input_mel \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    340\u001b[0m is_longer \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/ms-research/venv/lib/python3.10/site-packages/transformers/models/clap/feature_extraction_clap.py:335\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    331\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39masarray(raw_speech)]\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# convert to mel spectrogram, truncate and pad if needed.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_input_mel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnb_max_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m waveform \u001b[38;5;129;01min\u001b[39;00m raw_speech\n\u001b[1;32m    337\u001b[0m ]\n\u001b[1;32m    339\u001b[0m input_mel \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    340\u001b[0m is_longer \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/ms-research/venv/lib/python3.10/site-packages/transformers/models/clap/feature_extraction_clap.py:251\u001b[0m, in \u001b[0;36mClapFeatureExtractor._get_input_mel\u001b[0;34m(self, waveform, max_length, truncation, padding)\u001b[0m\n\u001b[1;32m    248\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(waveform, (\u001b[38;5;241m0\u001b[39m, max_length \u001b[38;5;241m-\u001b[39m waveform\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, constant_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m truncation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfusion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 251\u001b[0m     input_mel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_np_extract_fbank_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmel_filters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     input_mel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([input_mel, input_mel, input_mel, input_mel], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/ms-research/venv/lib/python3.10/site-packages/transformers/models/clap/feature_extraction_clap.py:164\u001b[0m, in \u001b[0;36mClapFeatureExtractor._np_extract_fbank_features\u001b[0;34m(self, waveform, mel_filters)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_np_extract_fbank_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: np\u001b[38;5;241m.\u001b[39marray, mel_filters: Optional[np\u001b[38;5;241m.\u001b[39marray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    Compute the log-mel spectrogram of the provided `waveform` using the Hann window. In CLAP, two different filter\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    banks are used depending on the truncation pattern:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m          implementation when the truncation mode is not `\"fusion\"`.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     log_mel_spectrogram \u001b[38;5;241m=\u001b[39m \u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfft_window_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhann\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfft_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmel_filters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmel_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_mel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m log_mel_spectrogram\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/ms-research/venv/lib/python3.10/site-packages/transformers/audio_utils.py:557\u001b[0m, in \u001b[0;36mspectrogram\u001b[0;34m(waveform, window, frame_length, hop_length, fft_length, power, center, pad_mode, onesided, preemphasis, mel_filters, mel_floor, log_mel, reference, min_value, db_range, remove_dc_offset, dtype)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# note: ** is much faster than np.power\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m power \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 557\u001b[0m     spectrogram \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspectrogram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m power\n\u001b[1;32m    559\u001b[0m spectrogram \u001b[38;5;241m=\u001b[39m spectrogram\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mel_filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3. Get your CLAP audio embedding\n",
    "audio, sr = sf.read(\"sounds/test_48k.wav\")\n",
    "clap_inputs = processor(audios=audio, sampling_rate=sr, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    audio_emb_clap = clap_model.get_audio_features(**clap_inputs).audio_embeds  # (1, clap_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Get your CLAP text embedding for the prompt\n",
    "txt = [\"A calm river at sunrise\"]  # example prompt\n",
    "txt_inputs = processor(text=txt, return_tensors=\"pt\", padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    text_emb_clap = clap_model.get_text_features(**txt_inputs).text_embeds    # (1, clap_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d047473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Project CLAP’s embed_dim → transformer hidden_size\n",
    "#    so we can inject it into prompt_embeds\n",
    "proj = torch.nn.Linear(audio_emb_clap.size(-1), pipe.transformer.config.hidden_size).to(device)\n",
    "proj_audio = proj(audio_emb_clap)  # (1, hidden)\n",
    "proj_text  = proj(text_emb_clap)    # (1, hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build the normal text prompt_embeds via the pipeline’s tokenizer+text_encoder\n",
    "tok = pipe.tokenizer(\n",
    "    txt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=pipe.tokenizer.model_max_length,\n",
    ").to(device)\n",
    "enc = pipe.text_encoder(**tok)\n",
    "prompt_embeds = enc.last_hidden_state  # (1, seq_len, hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35578f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Fuse in your CLAP audio information by simple addition\n",
    "#    (you could also concat+linear, or more complex fusion)\n",
    "fused_prompt_embeds = prompt_embeds + proj_audio.unsqueeze(1) + proj_text.unsqueeze(1)\n",
    "\n",
    "# 8. Generate, passing only prompt_embeds (no `prompt` string) :contentReference[oaicite:2]{index=2}\n",
    "out = pipe(\n",
    "    prompt_embeds=fused_prompt_embeds,\n",
    "    audio_end_in_s=5.0,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.0,\n",
    "    output_type=\"np\",\n",
    ")\n",
    "generated = out.audios[0]  # numpy array\n",
    "\n",
    "# 9. Save your result\n",
    "sf.write(\"clap_plus_text.wav\", generated, samplerate=pipe.vae.config.sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
