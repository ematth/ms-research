@article{article,
	author = {Fremerey, C and Damm, David and Müller, Meinard and Kurth, F and Clausen, Michael},
	year = {2009},
	month = {01},
	pages = {},
	title = {Handling Scanned Sheet Music and Audio Recordings in Digital Music Libraries}
}

@misc{oord2016wavenet,
    title={WaveNet: A Generative Model for Raw Audio}, 
    author={Aaron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alex Graves and Nal Kalchbrenner and Andrew Senior and Koray Kavukcuoglu},
    year={2016},
    eprint={1609.03499},
    archivePrefix={arXiv},
    primaryClass={cs.SD}
}

@article{contreras2023omrcnn,
	abstract = {The recognition of patterns that have a time dependency is common in areas like speech recognition or natural language processing. The equivalent situation in image analysis is present in tasks like text or video recognition. Recently, Convolutional Recurrent Neural Networks (CRNN) have been broadly applied to solve these tasks in an end-to-end fashion with successful performance. However, its application to Optical Music Recognition (OMR) is not so straightforward due to the presence of different elements sharing the same horizontal position, disrupting the linear flow of the timeline. In this paper, we study the ability of the state-of-the-art CRNN approach to learn codes that represent this disruption in homophonic scores. In our experiments, we study the lower bounds in the recognition task of real scores when the models are trained with synthetic data. Two relevant conclusions are drawn: (1) Our serialized ways of encoding the music content are appropriate for CRNN-based OMR; (2) the learning process is possible with synthetic data, but there exists a glass ceiling when recognizing real sheet music.},
	author = {Alfaro-Contreras, Mar{\'\i}a and I{\~n}esta, Jos{\'e}M. and Calvo-Zaragoza, Jorge},
	date = {2023/05/26},
	date-added = {2024-04-25 16:49:19 -0500},
	date-modified = {2024-04-25 16:49:19 -0500},
	doi = {10.1007/s13735-023-00278-5},
	id = {Alfaro-Contreras2023},
	isbn = {2192-662X},
	journal = {International Journal of Multimedia Information Retrieval},
	number = {1},
	pages = {12},
	title = {Optical music recognition for homophonic scores with neural networks and synthetic music generation},
	url = {https://doi.org/10.1007/s13735-023-00278-5},
	volume = {12},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1007/s13735-023-00278-5}
}

@article{contreras2023omrpiano,
	abstract = {End-to-end solutions have brought about significant advances in the field of Optical Music Recognition. These approaches directly provide the symbolic representation of a given image of a musical score. Despite this, several documents, such as pianoform musical scores, cannot yet benefit from these solutions since their structural complexity does not allow their effective transcription. This paper presents a neural method whose objective is to transcribe these musical scores in an end-to-end fashion. We also introduce the GrandStaff dataset, which contains 53,882 single-system piano scores in common western modern notation. The sources are encoded in both a standard digital music representation and its adaptation for current transcription technologies. The method proposed in this paper is trained and evaluated using this dataset. The results show that the approach presented is, for the first time, able to effectively transcribe pianoform notation in an end-to-end manner.},
	author = {R{\'\i}os-Vila, Antonio and Rizo, David and I{\~n}esta, Jos{\'e}M. and Calvo-Zaragoza, Jorge},
	date = {2023/09/01},
	date-added = {2024-04-25 16:50:39 -0500},
	date-modified = {2024-04-25 16:50:39 -0500},
	doi = {10.1007/s10032-023-00432-z},
	id = {R{\'\i}os-Vila2023},
	isbn = {1433-2825},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	number = {3},
	pages = {347--362},
	title = {End-to-end optical music recognition for pianoform sheet music},
	url = {https://doi.org/10.1007/s10032-023-00432-z},
	volume = {26},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1007/s10032-023-00432-z}
}

@misc{mayer2024practical,
      title={Practical End-to-End Optical Music Recognition for Pianoform Music}, 
      author={Jiří Mayer and Milan Straka and Jan Hajič jr. au2 and Pavel Pecina},
      year={2024},
      eprint={2403.13763},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{andrea2021note,
	author = {Andrea and Paoline and Zahra, Amalia},
	title = {Music note position recognition in optical music recognition using convolutional neural network},
	journal = {International Journal of Arts and Technology},
	volume = {13},
	number = {1},
	pages = {45-60},
	year = {2021},
	doi = {10.1504/IJART.2021.115764},
	URL = {https://www.inderscienceonline.com/doi/abs/10.1504/IJART.2021.115764}
}

@ARTICLE {shi2017crnn,
	author = {B. Shi and X. Bai and C. Yao},
	journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
	title = {An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition},
	year = {2017},
	volume = {39},
	number = {11},
	issn = {1939-3539},
	pages = {2298-2304},
	abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
	keywords = {feature extraction;text recognition;neural networks;image recognition;logic gates;convolutional codes;context},
	doi = {10.1109/TPAMI.2016.2646371},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {nov}
}

@misc{lilypond,
	key = {Lilypond},
	title = {Lilypond music notation for everyone},
	url = {https://lilypond.org/doc/v2.25/Documentation/web/index},
	journal = {Lilypond.org},
	year = {2024} 
}
